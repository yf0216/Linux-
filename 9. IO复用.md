	I/O复用使得程序能同时监听多个文件描述符，这对提高程序的性能至关重要。通常，网络程序在下列情况下需要使用I/O复用技术。

- 客户端程序要同时处理多个socket。
- 客户端程序要同时处理用户输入和网络连接。
- TCP服务器要同时处理监听socket和连接socket。
- 服务器要同时处理TCP请求和UDP请求。
- 服务器要同时监听多个端口，或者处理多种服务。

## 9.1 select 系统调用

​	select系统调用的用途是：在一段指定时间内，监听用户感兴趣的文件描述符上的可读、可写和异常等事件。

### 9.1.1 select API

​	select系统调用的原型如下：

```c++
#include <sys/select.h>
int select(int nfds,fd_set* readfds,fd_set* writefds,fd_set* exceptfds,struct timeval* timeout);
```

1. nfds参数指定被监听的文件描述符的总数。它通常被设置为select监听的所有文件描述符中的最大值加1，因为文件描述符是从0开始计数的。
2. readfds、writefds、exceptfds参数分别指向可读、可写和异常等事件对应的文件描述符集合。应用程序调用select函数时，通过这3个参数传入自己感兴趣的文件描述符。select调用返回时，内核将修改他们来通知应用程序哪些文件描述符已经就绪。这三个参数是fd_set结构指针类型。fd_set结构体的定义如下：

```c++
#include <typesizes.h>
#define __FD_SETSIZE 1024

#include <sys/select.h>
#define FD_SETSIZE __FD_SETSIZE
typedef long int __fd_mask;
#undef __NFDBITS
#define __NFDBITS  (8 * (int) sizeof (__fd_mask));
typedef struct
{
	#ifdef __USE_XOPEN __fd_mask fds_bits[ __FD_SETSIZE / __NFDBITS ];
    #define __FDS_BITS(set) ((set)->fds_bits)
    #else __fd_mask __fds_bits[ __FD_SETSIZE / __NFDBITS ];
    #define __FDS_BITS(set) ((set)->fds_bits)
    #endif
}fd_set;
```

​	由以上定义可见，fd_set结构体仅包含一个整型数组，该数组的每一位（bit）标记一个文件描述符。fd_set能容纳的文件描述符数量由FD_SETSIZE指定，这就限制了select能同时处理的文件描述符的总量。

​	由于位操作过于繁琐，我们应该使用下面的一系列宏来访问fd_set结构体中的位：

```c++
#include <sys/select.h>
FD_ZERO(fd_set *fdset);						/*清除fdset的所有位*/
FD_SET(int fd,fd_set *fdset);				/*设置fdset的位fd*/
FD_CLR(int fd,fd_set *fdset);				/*清除fdset的位fd*/
int FD_ISSET(int fd,fd_set *fdset);			/*测试fdset的位fd是否被设置*/
```

3.timeout参数用来设置select函数的超时时间。他是一个timeval结构类型的指针，采用指针参数是因为内核将修改它以告诉应用程序select等待了多久。不过我们不能完全信任select调用后返回的timeout值，比如调用失败时timeout值是不确定的。timeval结构体的定义如下：

```c++
struct timeval
{
    long tv_sec;	/*秒数*/
    long tv_usec;	/*微秒数*/
};
```

​	由以上定义可见，select给我们提供了一个微秒级的定时方式。如果给timeout变量的tv_sec成员和tv_usec成员都传递0，则select将立即返回。如果给timeout传递NULL，则select将一直阻塞，直到某个文件描述符就绪。

​	select成功时返回就绪（可读、可写和异常）文件描述符的总数。如果在超时时间内没有任何文件描述符就绪，select将返回0。select失败时返回-1并设置errno。如果在select等待期间，程序接收到信号，则select立即返回-1并设置errno为EINTR；

### 9.1.2 文件描述符就绪条件

​	哪些情况下文件描述符可以被认为是可读、可写或者出现异常，对于select的使用非常关键。在网络编程中，下列情况下socket可读：

- socket内核接收缓存区中的字节数大于或等于其低水位标记SO_RCVLOWAT。此时我们可以无阻塞地读该socket，并且读操作返回的字节数大于0。
- socket通信的对方关闭连接。此时对该socket的读操作将返回0。
- 监听socket上有新的连接请求。
- socket上有未处理的错误。此时我们可以使用getsockopt来读取和清楚该错误。

下列情况socket可写：

- socket内核发送缓冲区中的可用字节数大于或等于其低水位标记SO_SNDLOWAT。此时我们可以无阻塞地写该socket，并且写操作返回的字节数大于0。
- socket的写操作被关闭。对写操作被关闭的socket执行写操作将触发一个SIGPIPE信号。
- socket使用非阻塞connect连接成功或者失败之后。
- socket上有未处理的错误。此时我们可以使用getsockopt来读取和清除该错误。

网络程序中，select能处理的异常情况只有一种：socket上接收到带外数据。

## 9.2 poll 系统调用

​	poll系统调用和select类似，也是在是指定时间内轮询一定数量的文件描述符，以测试其中是否有就绪者。poll的原型如下：

```c++
#include <poll.h>
int poll(struct pollfd* fds,nfds_t nfds,int timeout);
```

1. fds参数是一个pollfd结构类型的数组，它指定所有我们感兴趣的文件描述符上发生的可读、可写和异常等事件。pollfd结构体的定义如下：

```c++
struct pollfd
{
    int fd;				/*文件描述符*/
    short events;		/*注册的事件*/
    short revents;		/*实际发生的事件，由内核填充*/
};
```

​	其中，fd成员指定文件描述符；events成员告诉poll监听fd上的哪些事件，它是一系列事件的按位或；revents成员则由内核修改，以通知应用程序fd上实际发生了哪些事件。poll支持的事件类型如下表所示。

<img src="https://github.com/yfwang0216/Linux-/blob/main/imgs/image-20210820150850963.png?raw=true" alt="image-20210820150850963" style="zoom:80%;" />

<img src="https://github.com/yfwang0216/Linux-/blob/main/imgs/image-20210820150911759.png?raw=true" alt="image-20210820150911759" style="zoom:80%;" />

​	上表中，POLLRDNORM、POLLRDBAND、POLLWRNORM、POLLWRBAND由XOPEN规范定义。它们实际上是将POLLIN事件和POLLOUT事件分得更细致，以区别对待普通数据和优先数据。但Linux并不完全支持它们。

2.nfds参数指定被监听事件集合fds的大小。其类型nfds_t的定义如下：

```c++
typedef unsigned long int nfds_t;
```

3.timeout参数指定poll的超时值，单位是毫秒。当timeout为-1时，poll调用将永远阻塞，直到某个事件发生；当timeout为0时，poll调用将立即返回。

​	poll系统调用的返回值的含义与select相同。

## 9.3 epoll 系列系统调用

### 9.3.1 内核事件表

​	epoll是Linux特有的I/O复用函数。它在实现和使用上与select、poll有很大差异。首先，epoll使用一组函数来完成任务，而不是单个函数。其次，epoll把用户关心的文件描述符上的事件放在内核里的一个事件表中，从而无需像select和poll那样每次调用都要重复传入文件描述符集或事件集。但epoll需要使用一个额外的文件描述符，来唯一标识内核中的这个事件表。这个文件描述符使用如下epoll_create函数来创建：

```c++
#include <sys/epoll.h>
int epoll_create(int size);
```

​	size参数现在并不起作用，只是给内核一个提示，告诉它事件表需要多大。该函数返回的文件描述符将用作其他所有epoll系统调用的第一个参数，以指定要访问的内核事件表。

​	下面的函数用来操作epoll的内核事件表：

```c++
#include <sys/epoll.h>
int epoll_ctl(int epfd,int op,int fd,struct epoll_event *event);
```

​	fd参数是要操作的文件描述符，op参数则指定操作类型。操作类型有如下三种：

- EPOLL_CTL_ADD,往事件表中注册fd上的事件。
- EPOLL_CTL_MOD,修改fd上的注册事件。
- EPOLL_CTL_DEL，删除fd上的注册事件。

event参数指定事件，它是epoll_event结构指针类型。epoll_event的定义如下：

```c++
struct epoll_event
{
    __unit32_t events;		/*epoll事件*/
    epoll_data_t data;		/*用户数据*/
};
```

​	其中events成员描述事件类型。epoll支持的事件类型和poll基本相同。表示epoll事件类型的宏是在poll对应的宏前加上“E”，比如epoll的数据可读事件是EPOLLIN。但epoll有两个额外的事件类型——EPOLLET和EPOLLONESHOT。它们对于epoll的高效运作非常关键。data成员用于存储用户数据，其类型epoll_data_t的定义如下：

```c++
typedef union epoll_data
{
    void* ptr;
    int fd;
    unit32_t u32;
    unit64_t u64;
}epoll_data_t;
```

​	epoll_data_t是一个联合体，其4个成员中使用最多的是fd，它指定事件所从属的目标文件描述符。ptr成员可用来指定与fd相关的用户数据。但由于epoll_data_t是一个联合体，我们不能同时使用其ptr成员和fd成员，因此，如果要将文件描述符和用户数据关联起来，以实现快速的数据访问，只能使用其他手段，比如放弃使用epoll_data_t的fd成员，而在ptr指向的用户数据中包含fd。

​	epoll_ctl成功时返回0，失败则返回-1并设置errno。

### 9.3.2 epoll_wait 函数

​	epoll系列系统调用的主要接口是epoll_wait函数。它在一段超时时间内等待一组文件描述符上的事件，其原型如下：

```c++
#include <sys/epoll.h>
int epoll_wait(int epfd,struct epoll_event* events,int maxevents,int timeout);
```

​	该函数成功时返回就绪的文件描述符的个数，失败时返回-1并设置errno。

​	关于该函数的参数，我们从后往前讨论。timeout参数的含义与poll接口的timeout参数相同。maxevents参数指定最多监听多少个事件，它必须大于0。

​	epoll_wait函数如果检测到事件，就将所有就绪的事件从内核事件表（由epfd参数指定）中复制到它的第二个参数events指向的数组中。这个数组只用于输出epoll_wait检测到的就绪事件，而不像select和poll的数组参数那样既用于传入用户注册的事件，又用于输出内核检测到的就绪事件。这就极大地提高了应用程序索引就绪文件描述符的效率。如下代码体现了差别：

```c++
/*如何索引poll返回的就绪文件描述符*/
int ret = poll(fds,MAX_EVENT_NUMBER,-1);
/*必须遍历所有已注册文件描述符并找到其中的就绪者（当然，可以利用ret来稍做优化）*/
for(int i = 0;i < MAX_EVENT_NUMBER,++i){
    if(fds[i].revent && POLLIN)
        int sockfd = fds[i].fd; //处理sockfd
}

/*如何索引epoll返回的就绪文件描述符*/
int ret = epoll_wait(epollfd,events,MAX_EVENT_NUMBER,-1);
/*仅遍历就绪的ret个文件描述符*/
for(int i = 0;i < ret;i++){
    int sockfd = events[i].data.fd;//sockfd肯定就绪，直接处理
}
```

### 9.3.3 LT 和 ET 模式

​	epoll对文件描述符的操作有两种模式：LT（Level Trigger,电平触发）模式和ET（Edge Trigger，边沿触发）模式。LT模式是默认的工作模式，这种模式下epoll相当于一个效率较高的poll。当往epoll内核事件表中注册一个文件描述符上的EPOLLET事件时，epoll将以ET模式来操作该文件描述符。ET模式是epoll的高效工作模式。

​	对于采用LT工作模式的文件描述符，当epoll_wait检测到其上有事件发生并将此事件通知应用程序后，应用程序可以不立即处理该事件.这样，当应用程序下一次调用epoll_wait时，epoll_wait还会再次向应用程序通告此事件，直到该事件被处理。而对于采用ET工作模式的文件描述符，当epoll_wait检测到其上有事件发生并将此事件通知应用程序后，应用程序必须立即处理该事件，因为后续的epoll_wait调用将不再向应用程序通知这一事件。可见，ET模式在很大程度上降低了同一个epoll事件被重复触发的次数，因此效率要比LT模式高。以下代码体现了LT和ET在工作方式上的差异。

```c++
#include <sys/types.h>
#include <sys/socket.h>
#include <netinet/in.h>
#include <arpa/inet.h>
#include <assert.h>
#include <stdio.h>
#include <unistd.h>
#include <errno.h>
#include <string.h>
#include <fcntl.h>
#include <stdlib.h>
#include <sys/epoll.h>
#include <pthread.h>
#include <iostream>

#define MAX_EVENT_NUMBER 1024
#define BUFFER_SIZE 10
/*将文件描述符设置成非阻塞的*/
int setnonblocking(int fd){
    int old_option = fcntl(fd,F_GETFL);
    int new_option = old_option | O_NONBLOCK;
    fcntl(fd,F_SETFL,new_option);
    return old_option;
}
/*将文件描述符fd上的EPOLLIN注册到epollfd指示的epoll内核事件表中，参数enable_et指定是否对fd启用ET模式*/
void addfd(int epollfd,int fd,bool enable_et){
    epoll_event event;
    event.data.fd = fd;
    event.events = EPOLLIN;
    if(enable_et){
        event.events |= EPOLLET;
    }
    epoll_ctl(epollfd,EPOLL_CTL_ADD,fd,&event);
    setnonblocking(fd);
}
/*LT模式的工作流程*/
void lt(epoll_event* events,int number,int epollfd,int listenfd){
    char buf[BUFFER_SIZE];
    for(int i = 0;i < number;i++){
        int sockfd = events[i].data.fd;
        if(sockfd == listenfd){
            struct sockaddr_in client_address;
            socklen_t client_addrlength = sizeof(client_address);
            int connfd = accept(listenfd,(struct sockaddr*)&client_address,&client_addrlength);
            addfd(epollfd,connfd,false);//对connfd禁用ET模式
        }else if(events[i].events && EPOLLIN){
            //只要socket读缓存中还有未读出的数据，这段代码就被触发
            std::cout<<"event trigger once"<<std::endl;
            memset(buf,'\0',BUFFER_SIZE);
            int ret = recv(sockfd,buf,BUFFER_SIZE-1,0);
            if(ret <= 0){
                close(sockfd);
                continue;
            }
            std::cout<<"get "<<ret<<" bytes of content: "<<buf<<std::endl;
        }else{
            std::cout<<"something else happened"<<std::endl;
        }
    }
}
/*ET模式的工作流程*/
void et(epoll_event* events,int number,int epollfd,int listenfd){
    char buf[BUFFER_SIZE];
    for(int i = 0;i < number;i++){
        int sockfd = events[i].data.fd;
        if(sockfd == listenfd){
            struct sockaddr_in client_address;
            socklen_t  client_addrlength = sizeof(client_address);
            int connfd = accept(listenfd,(struct sockaddr*)&client_address,&client_addrlength);
            addfd(epollfd,connfd,true);//对connfd开启ET模式
        }else if(events[i].events & EPOLLIN){
            //这段代码不会被重复触发，所以我们循环读取数据，以确保把socket读缓存中的所有数据读出
            std::cout<<"event trigger once"<<std::endl;
            while(1){
                memset(buf,'\0',BUFFER_SIZE);
                int ret = recv(sockfd,buf,BUFFER_SIZE-1,0);
                if(ret < 0){
                    //对于非阻塞IO，下面的条件成立表示数据已经全部读取完毕。此后，epoll就能再次触发sockfd上的EPOLLIN事件，以驱动下一次读操作
                    if(errno == EAGAIN || errno == EWOULDBLOCK){
                        std::cout<<"read later"<<std::endl;
                        break;
                    }
                    close(sockfd);
                    break;
                }else if(ret == 0) close(sockfd);
                else std::cout<<"get "<<ret<<" bytes of content: "<<buf<<std::endl;
            }
        }else std::cout<<"something else happened"<<std::endl;
    }
}

int main(int argc,char* argv[]){
    if(argc <= 2){
        std::cout<<"usage: "<<basename(argv[0])<<" ip_address port_number"<<std::endl;
        return 1;
    }
    const char* ip = argv[1];
    int port = atoi(argv[2]);

    int ret = 0;
    struct sockaddr_in address;
    bzero(&address,sizeof(address));
    address.sin_family = AF_INET;
    inet_pton(AF_INET,ip,&address.sin_addr);
    address.sin_port = htons(port);

    int listenfd = socket(PF_INET,SOCK_STREAM,0);
    assert(listenfd >= 0);

    ret = bind(listenfd,(struct sockaddr*)&address,sizeof(address));
    assert(ret != -1);

    ret = listen(listenfd,5);
    assert(ret != -1);

    epoll_event events[MAX_EVENT_NUMBER];
    int epollfd = epoll_create(5);
    assert(epollfd != -1);
    addfd(epollfd,listenfd,true);

    while(1){
        int ret = epoll_wait(epollfd,events,MAX_EVENT_NUMBER,-1);
        if(ret < 0){
            std::cout<<"epoll failure"<<std::endl;
            break;
        }

        lt(events,ret,epollfd,listenfd);    //使用LT模式
        //et(event,ret,epollfd,listenfd);   //使用ET模式
    }

    close(listenfd);
    return 0;
}
```

### 9.3.4 EPOLLONESHOT 事件

​	即使我们使用ET模式，一个socket上的某个事件还是可能被触发多次。这在并发程序中就会引起一个问题。比如一个线程在读取完某个socket上的数据后开始处理这些数据，而在数据的处理过程中该socket上又有新数据可读（EPOLLIN再次被触发），此时另外一个线程被唤醒来读取这些新的数据。于是就出现了两个线程同时操作一个socket的局面。这当然不是我们期望的。我们期望的是一个socket连接在任一时刻都只被一个线程处理。这一点可以使用epoll的EPOLLONESHOT事件实现。

​	对于注册了EPOLLONESHOT事件的文件描述符，操作系统最多触发其上注册的一个可读、可写或者异常事件，且只触发一次，除非我们使用epoll_ctl函数重置该文件描述符上注册的EPOLLONESHOT事件。这样，注册了EPOLLONESHOT事件的socket一旦被某个线程处理完毕，该线程就应该立即重置这个socket上的EPOLLONESHOT事件，以确保这个socket下一次可读时，其EPOLLIN事件能被触发，进而让其他工作线程有机会继续处理这个socket。

​	以下代码展示了EPOLLONESHOT事件的使用。

```c++
#include <sys/types.h>
#include <sys/socket.h>
#include <netinet/in.h>
#include <arpa/inet.h>
#include <assert.h>
#include <stdio.h>
#include <unistd.h>
#include <errno.h>
#include <string.h>
#include <fcntl.h>
#include <stdlib.h>
#include <sys/epoll.h>
#include <pthread.h>
#include <iostream>

#define MAX_EVENT_NUMBER 1024
#define BUFFER_SIZE 1024

struct fds{
    int epollfd;
    int sockfd;
};

int setnonblocking(int fd){
    int old_option = fcntl(fd,F_GETFL);
    int new_option = old_option | O_NONBLOCK;
    fcntl(fd,F_SETFL,new_option);
    return old_option;
}

/*将fd上的EPOLLIN和EPOLLET事件注册到epollfd指示的epoll内核事件表中，参数oneshot指定是否注册fd上的EPOLLONESHOT事件*/
void addfd(int epollfd,int fd,bool oneshot){
    epoll_event event;
    event.data.fd =fd;
    event.events = EPOLLIN | EPOLLET;
    if(oneshot){
        event.events |= EPOLLONESHOT;
    }
    epoll_ctl(epollfd,EPOLL_CTL_ADD,fd,&event);
    setnonblocking(fd);
}

/*重置fd上的事件。这样操作之后，尽管fd上的EPOLLONESHOT事件被注册，但是操作系统仍然会触发fd上的EPOLLIN事件，且只触发一次*/
void reset_oneshot(int epollfd,int fd){
    epoll_event event;
    event.data.fd = fd;
    event.events = EPOLLIN | EPOLLET | EPOLLONESHOT;
    epoll_ctl(epollfd,EPOLL_CTL_MOD,fd,&event);
}
/*工作线程*/
void* worker(void* arg){
    int sockfd = ((fds*)arg)->sockfd;
    int epollfd = ((fds*)arg)->epollfd;
    std::cout<<"start new thread to receive data on fd: "<<sockfd<<std::endl;
    char buf[BUFFER_SIZE];
    memset(buf,'\0',BUFFER_SIZE);
    /*循环读取sockfd上的数据，直到遇到EAGAIN错误*/
    while(1){
        int ret = recv(sockfd,buf,BUFFER_SIZE-1,0);
        if(ret == 0){
            close(sockfd);
            std::cout<<"foreiner closed the connection"<<std::endl;
            break;
        }else if(ret < 0){
            if(errno == EAGAIN){
                reset_oneshot(epollfd,sockfd);
                std::cout<<"read later"<<std::endl;
                break;
            }
        }else{
            std::cout<<"get content: "<<buf<<std::endl;
            /*休眠5s，模拟数据处理过程*/
            sleep(5);
        }
    }
    std::cout<<"end thread receiving data on fd: "<<sockfd<<std::endl;
}


int main(int argc,char* argv[]){
    if(argc <= 2){
        std::cout<<"usage: "<<basename(argv[0])<<" ip_address port_number"<<std::endl;
        return 1;
    }

    const char* ip = argv[1];
    int port = atoi(argv[2]);

    int ret = 0;
    struct sockaddr_in address;
    bzero(&address,sizeof(address));
    address.sin_family = AF_INET;
    inet_pton(AF_INET,ip,&address.sin_addr);
    address.sin_port = htons(port);

    int listenfd = socket(PF_INET,SOCK_STREAM,0);
    assert(listenfd >= 0);
    ret = bind(listenfd,(struct sockaddr*)&address,sizeof(address));
    assert(ret != -1);

    ret = listen(listenfd,5);
    assert(ret != -1);

    epoll_event events[MAX_EVENT_NUMBER];
    int epollfd = epoll_create(5);
    assert(epollfd != -1);
    /*注意，监听socket listenfd上是不能注册EPOLLONESHOT事件的，否则应用程序只能处理一个客户连接！因为后续的客户连接请求将不再触发listenfd上的EPOLLIN事件*/
    addfd(epollfd,listenfd,false);

    while(1){
        int ret = epoll_wait(epollfd,events,MAX_EVENT_NUMBER,-1);
        if(ret < 0){
            std::cout<<"epoll failure"<<std::endl;
            break;
        }

        for(int i = 0;i < ret;i++){
            int sockfd = events[i].data.fd;
            if(sockfd == listenfd){
                struct sockaddr_in client_address;
                socklen_t client_addrlength = sizeof(client_address);
                int connfd = accept(listenfd,(struct sockaddr*)&client_address,&client_addrlength);
                /*对每个非监听文件描述符都注册EPOLLONESHOT事件*/
                addfd(epollfd,connfd,true);
            }else if(events[i].events & EPOLLIN){
                pthread_t thread;
                fds fds_for_new_worker;
                fds_for_new_worker.epollfd = epollfd;
                fds_for_new_worker.sockfd = sockfd;
                /*新启动一个工作线程为sockfd服务*/
                pthread_create(&thread,NULL,worker,(void*)&fds_for_new_worker);
            }else{
                std::cout<<"something else happend"<<std::endl;
            }
        }
    }

    close(listenfd);
    return 0;
}

```

​	从工作线程worker来看，如果一个工作线程处理完某个socket上的一次请求（我们用休眠5s来模拟这个过程）之后，又接收到该socket上新的客户请求，则该线程将继续为这个socket服务。并且因为该socket上注册了EPOLLONESHOT事件，其他线程没有机会接触这个socket，如果工作线程等待5s后仍然没收到该socket上的下一批客户数据，则它将放弃为该socket服务。同时，它调用reset_oneshot函数来重置该socket上的注册事件，这将使epoll有机会再次检测到该socket上的EPOLLIN事件，进而使得其他线程有机会为该socket服务。

​	由此看来，尽管一个socket在不同时间可能被不同的线程处理，但同一时刻肯定只有一个线程在为它服务。这就保证了连接的完整性，从而避免了很多可能的竟态条件。

## 9.4 三组I/O复用函数的比较

​	

|                系统调用                |                            select                            |                             poll                             |                            epoll                             |
| :------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
|                事件集合                | 用户通过3个参数分别传入感兴趣的可读、可写及异常等事件，内核通过对这些参数的在线修改来反馈其中的就绪事件。这使得用户每次调用select都要重置这三个参数 | 统一处理所有事件类型，因此只需一个事件集参数。用户通过pollfd.events传入感兴趣的事件，内核通过修改pollfd.revents反馈其中的就绪事件 | 内核通过一个事件表直接管理用户感兴趣的所有事件。因此每次调用epoll_wait时，无需反复传入用户感兴趣的事件，epoll_wait系统调用的参数events仅用来反馈就绪的事件 |
| 应用程序索引就绪文件描述符的时间复杂度 |                             O(n)                             |                             O(n)                             |                             O(1)                             |
|          最大支持文件描述符数          |                        一般有最大限制                        |                            65535                             |                            65535                             |
|                工作模式                |                              LT                              |                              LT                              |                        支持ET高效模式                        |
|           内核实现和工作效率           |       采用轮询方式来检测就绪事件。算法时间复杂度为O(n)       |       采用轮询方式来检测就绪事件。算法时间复杂度为O(n)       |       采用回调方式来检测就绪事件，算法时间复杂度为O(1)       |



## 9.5 I/O复用的高级应用：聊天室程序

### 9.5.1 客户端

​	客户端程序使用poll同时监听用户输入和网络连接，并利用splice函数将用户输入内容直接定向到网络连接上以发送之，从而实现数据零拷贝，提高了程序执行效率。客户端程序代码清单如下：

```c++
#define _GNU_SOURCE 1
#include <sys/types.h>
#include <sys/socket.h>
#include <netinet/in.h>
#include <assert.h>
#include <arpa/inet.h>
#include <stdio.h>
#include <unistd.h>
#include <string.h>
#include <stdlib.h>
#include <poll.h>
#include <fcntl.h>

#define BUFFER_SIZE 64

int main(int argc,char* argv[]){
    if(argc <= 2){
        printf("usage: %s 1p_address port_number\n",basename(argv[0]));
        return 1;
    }
    const char* ip = argv[1];
    int port = atoi(argv[2]);

    struct sockaddr_in server_address;
    bzero(&server_address,sizeof(server_address));
    server_address.sin_family = AF_INET;
    inet_pton(AF_INET,ip,&server_address.sin_addr);
    server_address.sin_port = htons(port);

    int sockfd = socket(PF_INET,SOCK_STREAM,0);
    assert(sockfd >= 0);
    if(connect(sockfd,(struct sockaddr*)&server_address,sizeof(server_address)) < 0){
        printf("connection failed\n");
        close(sockfd);
        return 1;
    }

    pollfd fds[2];
    /*注册文件描述符0（标准输入）和文件描述符sockfd上的可读事件*/
    fds[0].fd = 0;
    fds[0].events = POLLIN;
    fds[0].revents = 0;
    fds[1].fd = sockfd;
    fds[1].events = POLLIN | POLLRDHUP;
    fds[1].revents = 0;

    char read_buf[BUFFER_SIZE];
    int pipefd[2];
    int ret = pipe(pipefd);
    assert(ret != -1);

    while(1){
        ret = poll(fds,2,-1);
        if(ret < 0){
            printf("poll failure\n");
            break;
        }

        if(fds[1].revents && POLLRDHUP){
            printf("server close the connection\n");
            break;
        }
        else if(fds[1].revents && POLLIN){
            memset(read_buf,'\0',BUFFER_SIZE);
            recv(fds[1].fd,read_buf,BUFFER_SIZE-1,0);
            printf("%s\n",read_buf);
        }

        if(fds[0].revents && POLLIN){
            /*使用splice将用户输入的数据直接写到sockfd上（零拷贝）*/
            ret = splice(0,NULL,pipefd[1],NULL,32768,SPLICE_F_MORE | SPLICE_F_MOVE);
            ret = splice(pipefd[0],NULL,sockfd,NULL,32768,SPLICE_F_MORE | SPLICE_F_MOVE);
        }
    }

    close(sockfd);
    return 0;
}
```

### 9.6.2 服务器

​	服务器程序使用poll同时管理监听socket和连接socket，并且使用牺牲空间换取时间的策略来提高服务器性能，代码如下所示：

```c++
#define _GNU_SOURCE 1
#include <sys/types.h>
#include <sys/socket.h>
#include <netinet/in.h>
#include <assert.h>
#include <arpa/inet.h>
#include <stdio.h>
#include <unistd.h>
#include <string.h>
#include <stdlib.h>
#include <poll.h>
#include <fcntl.h>
#include <errno.h>

#define USER_LIMIT 5 /*最大用户数量*/
#define BUFFER_SIZE 64 /*读缓冲区的大小*/
#define FD_LIMLT 65535 /*文件描述符数量限制*/
/*客户数据：客户端socket地址、待写到客户端的数据的位置、从客户端读入的数据*/
struct client_data{
    sockaddr_in address;
    char* write_buf;
    char buf[BUFFER_SIZE];
};

int setnonblocking(int fd){
    int old_option = fcntl(fd,F_GETFL);
    int new_option = old_option | O_NONBLOCK;
    fcntl(fd,F_SETFL,new_option);
    return old_option;
}

int main(int argc,char* argv[]){
    if(argc <= 2){
        printf("usage: %s ip_address port_number\n",basename(argv[0]));
        return 1;
    }
    const char* ip = argv[1];
    int port = atoi(argv[2]);

    int ret = 0;
    struct sockaddr_in address;
    bzero(&address,sizeof(address));
    address.sin_family = AF_INET;
    inet_pton(AF_INET,ip,&address.sin_addr);
    address.sin_port = htons(port);

    int listenfd = socket(PF_INET,SOCK_STREAM,0);
    assert(listenfd >= 0);

    ret = bind(listenfd,(struct sockaddr*)&address,sizeof(address));
    assert(ret != -1);

    ret = listen(listenfd,5);
    assert(ret != -1);
    
    /*创建user数组，分配FD_LIMLT个client_data对象，可以预期：每个可能的socket连接
    都可以获得一个这样的对象，并且socket的值可以直接用来索引（作为数组下标）socket连接对应的
    client_data对象，这是将socket和客户数据关联的简单而高效的方式*/
    client_data* users = new client_data[FD_LIMLT];
    /*尽管我们分配了足够多的client_data对象，但为了提高poll的性能，仍然有必要限制用户的数量*/
    pollfd fds[USER_LIMIT+1];
    int user_counter = 0;
    for(int i = 1;i <= USER_LIMIT;i++){
        fds[i].fd = -1;
        fds[i].events = 0;
    }

    fds[0].fd = listenfd;
    fds[0].events = POLLIN | POLLERR;
    fds[0].revents = 0;

    while(1){
        ret = poll(fds,user_counter+1,-1);
        if(ret < 0){
            printf("poll failure\n");
            break;
        }

        for(int i = 0;i < user_counter+1;i++){
            if((fds[i].fd == listenfd) && (fds[i].revents & POLLIN)){
                struct sockaddr_in client_address;
                socklen_t client_addrlength = sizeof(client_address);
                int connfd = accept(listenfd,(struct sockaddr*)&client_address,&client_addrlength);
                if(connfd < 0){
                    printf("errno is %d\n",errno);
                    continue;
                }
                /*如果请求太多，则关闭新到的连接*/
                if(user_counter >= USER_LIMIT){
                    const char* info = "too many users\n";
                    printf("%s",info);
                    send(connfd,info,strlen(info),0);
                    close(connfd);
                    continue;
                }
                /*对于新的连接，同时修改fds和users数组，前文已经提到，users[connfd]
                对应于新连接文件描述符connfd的客户数据*/
                user_counter++;
                users[connfd].address = client_address;
                setnonblocking(connfd);
                fds[user_counter].fd = connfd;
                fds[user_counter].events = POLLIN | POLLRDHUP | POLLERR;
                fds[user_counter].revents = 0;
                printf("comes a new user,now have %d users\n",user_counter);    
            }
            else if(fds[i].revents && POLLERR){
                printf("get an error from %d\n",fds[i].fd);
                char errors[100];
                memset(errors,'\0',100);
                socklen_t length = sizeof(errors);
                if(getsockopt(fds[i].fd,SOL_SOCKET,SO_ERROR,&errors,&length) < 0){
                    printf("get socket option failed\n");
                }
                continue;
            }
            else if(fds[i].revents & POLLRDHUP){
                /*如果客户端关闭连接，则服务器也关闭对应的连接，并将用户总数-1*/
                users[fds[i].fd] = users[fds[user_counter].fd];
                close(fds[i].fd);
                fds[i] = fds[user_counter];
                i--;
                user_counter--;
                printf("a client left\n");
            }
            else if(fds[i].revents & POLLIN){
                int connfd = fds[i].fd;
                memset(users[connfd].buf,'\0',BUFFER_SIZE);
                ret = recv(connfd,users[connfd].buf,BUFFER_SIZE-1,0);
                printf("get %d bytes of client data %s feom %d\n",ret,users[connfd].buf,connfd);
                if(ret < 0){
                    /*如果读操作出错，则关闭连接*/
                    if(errno != EAGAIN){
                        close(connfd);
                        users[fds[i].fd] = users[fds[user_counter].fd];
                        fds[i] = fds[user_counter];
                        i--;
                        user_counter--;
                    }
                }
                else if(ret == 0){}
                else{
                    /*如果收到客户数据，则通知其他socket连接准备写数据*/
                    for(int j = 1;j <= user_counter;j++){
                        if(fds[i].fd == connfd){
                            continue;
                        }
                        fds[j].events |= ~POLLIN;
                        fds[j].events |= POLLOUT;
                        users[fds[j].fd].write_buf = users[connfd].buf;
                    }
                }
            }
            else if(fds[i].revents & POLLOUT){
                int connfd = fds[i].fd;
                if(!users[connfd].write_buf){
                    continue;
                }
                ret = send(connfd,users[connfd].write_buf,strlen(users[connfd].write_buf),0);
                users[connfd].write_buf = NULL;
                /*写完数据后需要重新注册fds[i]上的可读事件*/
                fds[i].events |= ~POLLOUT;
                fds[i].events |= POLLIN;
            }
        }
    }

    delete[] users;
    close(listenfd);
    return 0;
}
```

