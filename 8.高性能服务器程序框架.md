## 8.1 服务器模型

### 8.1.1 C/S 模型

​	C/S模型：所有客户端都通过访问服务器来获取所需的资源。

<img src="https://github.com/yfwang0216/Linux-/blob/main/imgs/image-20210816162216353.png?raw=true" alt="image-20210816162216353" style="zoom: 80%;" />

​	采用C/S模型的TCP服务器和TCP客户端的工作流程如图8-2所示。

​	C/S模型的逻辑很简单。服务器启动后，首先创建一个或多个监听socket，并调用bind函数将其绑定到服务器感兴趣的端口上，然后调用listen函数等待客户连接。服务器稳定运行之后，客户端就可以调用connect函数向服务器发起连接了。由于客户连接请求是随机到达的异步事件，服务器需要使用某种I/O模型来监听这一事件。I/O模型有多种，图中，服务器使用的是I/O复用技术之一的select系统调用。当监听到连接请求后，服务器就调用accept函数接受它，并分配一个逻辑单元为新的连接服务。逻辑单元可以是新创建的子进程、子线程或者其他。图中，服务器给客户端分配的逻辑单元是由fork系统调用创建的子进程。逻辑单元读取客户请求，处理该请求，然后将处理结果返回给客户端。客户端接收到服务器反馈的结果之后，可以继续向服务器发送请求，也可以立即主动关闭连接。如果客户端主动关闭连接，则服务器执行被动关闭连接。至此，双方的通信结束。需要注意的是，服务器在处理一个客户请求的同时还会继续监听其他客户请求，否则就变成了效率低下的串行服务器了。图中服务器同时监听多个客户端请求是通过select系统调用实现的。

<img src="https://github.com/yfwang0216/Linux-/blob/main/imgs/image-20210816162242667.png?raw=true" alt="image-20210816162242667" style="zoom:80%;" />



​	C/S模型非常适合资源相对集中的场合，并且它的实现也很简单，但其缺点也很明显：服务器是通信的中心，当访问量过大时，可能所有客户都将得到很慢的响应。下面的P2P模型解决了这个问题。

### 8.1.2 P2P 模型

​	P2P（Peer to Peer，点对点)模型比C/S模型更符合网络通信的实际情况，它摒弃了以服务器为中心的格局，让网络上所有主机重新回归对等的地位。

​	P2P模型使得每台机器在消耗服务的同时也给别人提供服务，这样资源能够充分、自由地共享。P2P模型的缺点也很明显：当用户之间传输的请求过多时，网络的负载将加重。

​	图a中所示的P2P模型存在一个显著的问题，即主机之间很难互相发现。所以实际使用的P2P模型通常带有一个专门的发现服务器，如图b所示。这个发现服务器通常还提供查找服务，使每个客户都能尽快地找到自己需要的资源。

![image-20210816185226809](https://github.com/yfwang0216/Linux-/blob/main/imgs/image-20210816185226809.png?raw=true)

​	从编程角度来讲，P2P模型可以看作C/S模型的扩展：每台主机既是客户端，又是服务器。

## 8.2 服务器编程框架

​	服务器的基本框架如图所示：

![image-20210817150002087](https://github.com/yfwang0216/Linux-/blob/main/imgs/image-20210817150002087.png?raw=true)

​	该图既能用来描述一台服务器，也能用来描述一个服务器机群。两种情况下各个部件的含义和功能如下表所示。

|     模块     |       单个服务器程序       |          服务器机群          |
| :----------: | :------------------------: | :--------------------------: |
| I/O处理单元  | 处理客户连接，读写网络数据 | 作为接入服务器，实现负载均衡 |
|   逻辑单元   |       业务进程或线程       |          逻辑服务器          |
| 网络存储单元 |   本地数据库、文件或缓存   |         数据库服务器         |
|   请求队列   |    各单元之间的通信方式    |  各服务器之间的永久TCP连接   |

​	I/O处理单元是服务器管理客户连接的模块。它通常要完成以下工作：等待并接受新的客户连接，接收客户数据，将服务器响应数据返回给客户端。但是，数据的收发不一定在I/O处理单元中执行，也可能在逻辑单元中执行，具体在何处执行取决于事件处理模式。对于一个服务器机群来说，I/O处理单元是一个专门的接入服务器。它实现负载均衡，从所有逻辑服务器中选取负荷最小的一台来为新客户服务。

​	一个逻辑单元通常是一个进程或线程。它分析并处理客户数据，然后将结果传递给I/O处理单元或者直接发送给客户端。对于服务器机群而言，一个逻辑单元本身就是一台逻辑服务器。服务器通常拥有多个逻辑单元，以实现对多个客户任务的并行处理。

​	网络存储单元可以是数据库、缓存和文件，甚至是一台独立的服务器。但它不是必须的，比如ssh、telnet等登陆服务就不需要这个单元。

​	请求队列是个各单元之间的通信方式的抽象。I/O处理单元接收到客户请求时，需要以某种方式通知一个逻辑单元来处理该请求。同样，多个逻辑单元同时访问一个存储单元时，也需要采用某种机制来协调处理竞态条件。请求队列通常被实现为池的一部分。对于服务器机群而言，请求队列是各台服务器之间预先建立的、静态的、永久的TCP连接。这种TCP连接能提高服务器之间交换数据的效率，因为它避免了动态建立TCP连接导致的额外的系统开销。

## 8.3 I/O模型

​	同步I/O模型要求用户代码自行执行I/O操作（将数据从内核缓冲区读入用户缓冲区，或将数据从用户缓冲区写入内核缓冲区），而异步I/O机制则由内核来执行I/O操作（数据在内核缓冲区和用户缓冲区之间的移动是由内核在“后台”完成的）。可以这么认为，同步I/O向应用程序通知的是I/O就绪事件，而异步I/O向应用程序通知的是I/O完成事件。几种I/O模型的差异如下表所示：

|  I/O模型  |                      读写操作和阻塞阶段                      |
| :-------: | :----------------------------------------------------------: |
|  阻塞I/O  |                      程序阻塞于读写函数                      |
|  I/O复用  | 程序阻塞于I/O复用系统调用，但可同时监听多个I/O事件。对I/O本身的读写操作是非阻塞的 |
| SIGIO信号 | 信号触发读写就绪事件，用户程序执行读写操作。程序没有阻塞阶段 |
|  异步I/O  |     内核执行读写操作并触发读写完成事件。程序没有阻塞阶段     |

## 8.4 两种高效的事件处理模式

​	服务器程序通常需要处理三类事件：I/O事件、信号及定时事件。接下来介绍两种高效的事件处理模式：Reactor和Proactor。

​	同步I/O模型通常用于实现Reactor模式，异步I/O模型则用于实现Proactor模式。不过后面我们将看到，如何使用同步I/O方式模拟出Proactor模式。

### 8.4.1 Reactor 模式

​	Reactor模式是这样一种模式，它要求主线程（I/O处理单元，下同）只负责监听文件描述上是否有事件发生，有的话就立即将该事件通知工作线程（逻辑单元，下同）。除此之外，主线程不做任何其他实质性的工作。读写数据，接受新的连接，以及处理客户请求均在工作线程中完成。

​	使用同步I/O模型（以epoll_wait为例）实现的Reactor模式的工作流程是：

1. 主线程往epoll内核事件表中注册socket上的读就绪事件。
2. 主线程调用epoll_wait等待socket上有数据可读。
3. 当socket上有数据可读时，epoll_wait通知主线程。主线程则将socket可读事件放入请求队列。
4. 睡眠在请求队列上的某个工作进程被唤醒，它从socket读取数据，并处理客户请求，然后往epoll内核事件表中注册该socket上的写就绪事件。
5. 主线程调用epoll_wait等待socket可写。
6. 当socket可写时，epoll_wait通知主线程。主线程将socket可写事件放入请求队列。
7. 睡眠在请求队列上的某个工作线程被唤醒，它往socket上写入服务器处理客户请求的结果。

下面是工作流程图：

![image-20210817182732462](https://github.com/yfwang0216/Linux-/blob/main/imgs/image-20210817182732462.png?raw=true)

​	图中，工作线程从请求队列中取出事件后，将根据事件的类型来决定如何处理它：对于可读事件，执行读数据和处理请求的操作；对于可写事件，执行写数据的操作。因此，图中所示的Reactor模式中，没必要区分所谓的“读工作线程”和“写工作线程”。

### 8.4.2 Proactor 模式

​	与Reactor模式不同，Proactor模式将所有I/O操作都交给主线程和内核来处理，工作线程仅仅负责业务逻辑。因此，Proactor模式更符合之前所描述的服务器编程框架。

​	使用异步I/O模型（以aio_read和aio_write为例）实现的Proactor模式的工作流程是：

1. 主线程调用aio_read函数向内核注册socket上的读完成事件，并告诉内核用户读缓存区的位置，以及读操作完成时如何通知应用程序（这里以信号为例）。
2. 主线程继续处理其他逻辑。
3. 当socket上的数据被读入用户缓冲区后，内存将向应用程序发送一个信号，以通知应用程序数据已经可用。
4. 应用程序预先定义好的信号处理函数选择一个工作线程来处理客户请求。工作线程处理完客户请求之后，调用aio_write函数向内核注册socket上的写完成事件，并告诉内核用户写缓冲区的位置，以及写操作完成时如何通知应用程序。
5. 主线程继续处理其他逻辑。
6. 当用户缓冲区的数据被写入socket之后，内核将向应用程序发送一个信号，以通知应用程序数据已经发送完毕。
7. 应用程序预先定义好的信号处理函数选择一个工作线程来做善后处理，比如决定是否关闭socket。

下图总结了Proactor模式的工作流程。

![image-20210818133400061](https://github.com/yfwang0216/Linux-/blob/main/imgs/image-20210818133400061.png?raw=true)

​		在图中，连接socket上的读写事件是通过aio_read/aio_write向内核注册的，因此内核将通过信号来向应用程序报告连接socket上的读写事件。所以，主线程中的epoll_wait调用仅能用来检测监听socket上的连接请求事件，而不能用来检测连接socket上的读写事件。

### 8.4.3 模拟Proactor 模式

​	使用同步I/O方式模拟出Proactor模式的一种方法。其原理是：主线程执行数据读写操作，读写完成之后，主线程向工作线程通知这一“完成事件”。那么从工作线程的角度来看，它们就直接获得了数据读写的结果，接下来要做的只是对读写的结果进行逻辑处理。

​	使用同步I/O模型（仍以epoll_wait为例）模拟出的Proactor模式的工作流程如下：

1. 主线程往epoll内核事件表中注册socket上的读就绪事件。
2. 主线程调用epoll_wait等待socket上有数据可读。
3. 当socket上有数据可读时，epoll_wait通知主线程。主线程从socket循环读取数据，直到没有更多数据可读，然后将读取到的数据封装成一个请求对象并插入请求队列。
4. 睡眠在请求队列上的某个工作线程被唤醒，它获得请求对象并处理客户请求，然后往epoll内核时间表中注册socket上的写就绪事件。
5. 主线程调用epoll_wait等待socket可写。
6. 当socket可写时，epoll_wait通知主线程。主线程往socket上写入服务器处理客户请求的结果。

下图总结了用同步I/O模型模拟出的Proactor模式的工作流程。

<img src="https://github.com/yfwang0216/Linux-/blob/main/imgs/image-20210818150957577.png?raw=true" style="zoom:80%;" />

## 8.5 两种高效的并发模式

### 8.5.1 半同步/半异步模式

​	在I/O模型中，“同步”和“异步”区分的是内核向应用程序通知的是何种I/O事件（是就绪事件还是完成事件），以及该由谁来完成I/O读写（是应用程序还是内核）。在并发模式中，“同步”指的是程序完全按照代码序列的顺序执行：“异步”指的是程序的执行需要由系统事件来驱动。常见的系统事件包括中断、信号等。比如，图a描述了同步的读操作，而图b则描述了异步的读操作。

<img src="https://github.com/yfwang0216/Linux-/blob/main/imgs/image-20210818154823063.png?raw=true" alt="image-20210818154823063" style="zoom:80%;" />

​	异步线程的执行效率高，实时性强。但是编写以异步方式执行的程序相对复杂，难于调试和扩展，而且不适合于大量的并发。而同步线程则相反，它虽然效率相对较低，实时性较差，但逻辑简单。因此，对于像服务器这种既要求较好的实时性，又要求能同时处理多个客户请求的应用程序，我们就应该同时使用同步线程和异步线程来实现，即采用半同步/半异步模式来实现。

​	半同步/半异步模式中，同步线程用于处理客户逻辑，相当于逻辑单元；异步线程用于处理I/O事件，相当于I/O处理单元。异步线程监听到客户请求后，就将其封装成请求对象并插入请求队列中。请求队列将通知某个工作在同步模式的工作线程来读取并处理该请求对象。具体选择哪个工作线程来为新的客户请求服务，则取决于请求队列的设计。工作流程如下图。

<img src="https://github.com/yfwang0216/Linux-/blob/main/imgs/image-20210818155458486.png?raw=true" alt="image-20210818155458486" style="zoom:80%;" />

​	有一种变体为半同步/半反应堆模式，如下图：

<img src="https://github.com/yfwang0216/Linux-/blob/main/imgs/image-20210818160352990.png?raw=true" alt="image-20210818160352990" style="zoom:80%;" />

​	半同步/半反应堆模式存在如下缺点：

- 主线程和工作线程共享请求队列。主线程往请求队列中添加任务，或者工作线程从请求队列中取出任务，都需要对请求队列加锁保护，从而白白耗费CPU时间。
- 每个工作线程在同一时间只能处理一个客户请求。如果客户数量较多，而工作线程较少，则请求队列中将堆积很多任务对象，客户端的响应速度将越来越慢。如果通过增加工作线程来解决这一问题，则工作线程的切换也将耗费大量CPU时间。

下图描述了一种相对高效的半同步/半异步模式，他的每个工作线程都能同时处理多个客户连接。

<img src="https://github.com/yfwang0216/Linux-/blob/main/imgs/image-20210818160833836.png?raw=true" alt="image-20210818160833836" style="zoom:80%;" />

### 8.5.2 领导者/追随者模式

​	领导者/追随者模式是多个工作线程轮流获得事件源集合，轮流监听、分发并处理事件的一种模式。在任意时间点，程序都仅有一个领导者线程，它负责监听I/O事件。而其他线程则都是追随者，他们休眠在线程池中等待成为新的领导者。当前的领导者如果检测到I/O事件，首先要从线程池中推选出新的领导者线程，然后处理I/O事件。此时，新的领导者等待新的I/O事件，而原来的领导者则处理I/O事件，二者实现了并发。

​	领导者/追随者模式包含如下几个组件：句柄集（HandleSet）、线程集（ThreadSet）、事件处理器（EventHandler）和具体的事件处理器（ConcreteEventHandler）。他们的关系如图所示。

<img src="https://github.com/yfwang0216/Linux-/blob/main/imgs/image-20210818163627282.png?raw=true" alt="image-20210818163627282" style="zoom:80%;" />

1.**句柄集**

​	句柄用于表示I/O资源，在Linux下通常就是一个文件描述符。句柄集管理众多句柄。它使用wait_for_event方法来监听这些句柄上的I/O事件，并将其中的就绪事件通知给领导者线程。领导者则调用绑定到Handle上的事件处理器来处理事件。领导者将Handle和事件处理器绑定是通过调用句柄集中的register_handle方法实现的。

2.**线程集**

​	这个组件是所有工作线程（包括领导者线程和追随者线程）的管理者。它负责各线程之间的同步，以及新领导者线程的推选。线程集中的线程在任一时间必处于如下三种状态之一：

- Leader：线程当前处于领导者身份，负责等待句柄集上的I/O事件。
- Processing：线程正在处理事件。领导者检测到I/O事件之后，可以转移到Processing状态来处理该事件，并调用promote_new_leader方法推选新的领导者；也可以指定其他追随者来处理事件（Event Handoff），此时领导者的地位不变。当处于Processing状态的线程处理完事件之后，如果当前线程集中没有领导者，则它将成为新的领导者，否则它就直接转变为追随者。
- Follower：线程当前处于追随者身份，通过调用线程集的join方法等待成为新的领导者，也可能被当前的领导者指定来处理新的任务。

下图显示了这三种状态之间的转换关系：

<img src="https://github.com/yfwang0216/Linux-/blob/main/imgs/image-20210818165622990.png?raw=true" alt="image-20210818165622990" style="zoom:80%;" />

3.**事件处理器和具体的事件处理器**

​	事件处理器通常包含一个或多个回调函数handle_event。这些回调函数用于处理事件对应的业务逻辑。事件处理器在使用前需要被绑定到某个句柄上，当该句柄上有事件发生时，领导者就执行与之绑定的事件处理器中的回调函数。具体的事件处理器是事件处理器的派生类。他们必须重新实现基类的handle_event方法，以处理特定的任务。

​	领导者/追随者模式的工作流程如下图：

<img src="https://github.com/yfwang0216/Linux-/blob/main/imgs/image-20210818170642460.png?raw=true" alt="image-20210818170642460" style="zoom:80%;" />

​	领导者/追随者模式有一个明显的缺点就是仅支持一个事件源集合，因此也无法让每个工作线程独立地管理多个客户连接。

## 8.6 有限状态机

## 8.7 提供服务器性能的其他建议

### 8.7.1 池

​	池是一组资源的集合，这组资源在服务器启动之初就被完全创建好并初始化，这称为静态资源分配。当服务器进入正式运行阶段，即开始处理客户请求的时候，如果它需要相关的资源，就可以直接从池中获取，无需动态分配。从最终的效果来看，池相当于服务器管理系统资源的应用层设施，它避免了服务器对内核的频繁访问。

​	根据不同的资源类型，池可分为多种，常见的有内存池、进程池、线程池和连接池。

​	内存池通常用于socket的接收缓存和发送缓存。对于某些长度有限的客户请求，比如HTTP请求，预先分配一个大小足够的接收缓存区是很合理的。当客户请求的长度超过接收缓冲区的大小时，我们可以选择丢弃请求或者动态扩大接收缓冲区。

​	进程池和线程池都是并发编程常用的“伎俩”。当我们需要一个工作进程或工作线程来处理新到来的客户请求时，我们可以直接从进程池或线程池中取得一个执行实体，而无需动态地调用fork或pthread_create等函数来创建进程和线程。

​	连接池通常用于服务器或服务器机群的内部永久连接。每个逻辑单元可能都需要频繁地访问本地的某个数据库。简单的做法是：逻辑单元每次需要访问数据库的时候，就向数据库程序发起连接，而访问完毕后释放连接。很显然，这种做法的效率太低。一种解决方案是使用连接池。连接池是服务器预先和数据库程序建立的一组连接的集合。当某个逻辑单元需要访问数据库时，它可以直接从连接池中取得一个连接的实体并使用之。待完成数据库的访问之后，逻辑单元再将该连接返还给连接池。

### 8.7.2 数据复制

​	如果内核可以直接处理从socket或者文件读入的数据，则应用程序就没必要将这些数据从内核缓冲区复制到应用程序缓冲区中。此外，用户代码内部（不访问内核）的数据复制也是应该避免的。

### 8.7.3 上下文切换和锁

​	并发线程必须考虑上下文切换的问题，即进程切换或线程切换导致的系统开销。并发程序需要考虑的另外一个问题是共享资源的加锁保护。
